{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!!!\n",
      "53450\n",
      "====================\n",
      "['On', 'the', 'other', 'hand', 'UNK', 'it', \"'s\", 'turning', 'out', 'to', 'be', 'another', 'very', 'bad', 'financial']\n",
      "====================\n",
      "2176\n",
      "====================\n",
      "====================\n",
      "24.563419117647058\n",
      "====================\n",
      "['ABC19980108.1830.0711', '0', '0', 'On', 0]\n",
      "['ABC19980108.1830.0711', '0', '1', 'the', 0]\n",
      "['ABC19980108.1830.0711', '0', '2', 'other', 0]\n",
      "['ABC19980108.1830.0711', '0', '3', 'hand', 0]\n",
      "['ABC19980108.1830.0711', '0', '4', ',', 0]\n",
      "['ABC19980108.1830.0711', '0', '5', 'it', 0]\n",
      "['ABC19980108.1830.0711', '0', '6', \"'s\", 0]\n",
      "['ABC19980108.1830.0711', '0', '7', 'turning', 0]\n",
      "['ABC19980108.1830.0711', '0', '8', 'out', 0]\n",
      "['ABC19980108.1830.0711', '0', '9', 'to', 0]\n",
      "['ABC19980108.1830.0711', '0', '10', 'be', 0]\n",
      "['ABC19980108.1830.0711', '0', '11', 'another', 0]\n",
      "['ABC19980108.1830.0711', '0', '12', 'very', 0]\n",
      "['ABC19980108.1830.0711', '0', '13', 'bad', 0]\n",
      "['ABC19980108.1830.0711', '0', '14', 'financial', 0]\n",
      "['ABC19980108.1830.0711', '0', '15', 'week', 1]\n",
      "['ABC19980108.1830.0711', '0', '16', 'for', 0]\n",
      "['ABC19980108.1830.0711', '0', '17', 'Asia', 0]\n",
      "['ABC19980108.1830.0711', '0', '18', '.', 0]\n",
      "['ABC19980108.1830.0711', '1', '0', 'The', 0]\n",
      "['ABC19980108.1830.0711', '1', '1', 'financial', 0]\n",
      "['ABC19980108.1830.0711', '1', '2', 'assistance', 0]\n",
      "['ABC19980108.1830.0711', '1', '3', 'from', 0]\n",
      "['ABC19980108.1830.0711', '1', '4', 'the', 0]\n",
      "['ABC19980108.1830.0711', '1', '5', 'World', 0]\n",
      "['ABC19980108.1830.0711', '1', '6', 'Bank', 0]\n",
      "['ABC19980108.1830.0711', '1', '7', 'and', 0]\n",
      "['ABC19980108.1830.0711', '1', '8', 'the', 0]\n",
      "['ABC19980108.1830.0711', '1', '9', 'International', 0]\n",
      "['ABC19980108.1830.0711', '1', '10', 'Monetary', 0]\n",
      "['ABC19980108.1830.0711', '1', '11', 'Fund', 0]\n",
      "['ABC19980108.1830.0711', '1', '12', 'are', 0]\n",
      "['ABC19980108.1830.0711', '1', '13', 'not', 0]\n",
      "['ABC19980108.1830.0711', '1', '14', 'helping', 0]\n",
      "['ABC19980108.1830.0711', '1', '15', '.', 0]\n",
      "['ABC19980108.1830.0711', '2', '0', 'In', 0]\n",
      "['ABC19980108.1830.0711', '2', '1', 'the', 1]\n",
      "['ABC19980108.1830.0711', '2', '2', 'last', 1]\n",
      "['ABC19980108.1830.0711', '2', '3', 'twenty', 1]\n",
      "['ABC19980108.1830.0711', '2', '4', 'four', 1]\n",
      "['ABC19980108.1830.0711', '2', '5', 'hours', 1]\n",
      "['ABC19980108.1830.0711', '2', '6', ',', 0]\n",
      "['ABC19980108.1830.0711', '2', '7', 'the', 0]\n",
      "['ABC19980108.1830.0711', '2', '8', 'value', 0]\n",
      "['ABC19980108.1830.0711', '2', '9', 'of', 0]\n",
      "['ABC19980108.1830.0711', '2', '10', 'the', 0]\n",
      "['ABC19980108.1830.0711', '2', '11', 'Indonesian', 0]\n",
      "['ABC19980108.1830.0711', '2', '12', 'stock', 0]\n",
      "['ABC19980108.1830.0711', '2', '13', 'market', 0]\n",
      "['ABC19980108.1830.0711', '2', '14', 'has', 0]\n",
      "['ABC19980108.1830.0711', '2', '15', 'fallen', 0]\n",
      "['ABC19980108.1830.0711', '2', '16', 'by', 0]\n",
      "['ABC19980108.1830.0711', '2', '17', 'twelve', 0]\n",
      "['ABC19980108.1830.0711', '2', '18', 'percent', 0]\n",
      "['ABC19980108.1830.0711', '2', '19', '.', 0]\n",
      "['ABC19980108.1830.0711', '3', '0', 'The', 0]\n",
      "['ABC19980108.1830.0711', '3', '1', 'Indonesian', 0]\n",
      "['ABC19980108.1830.0711', '3', '2', 'currency', 0]\n",
      "['ABC19980108.1830.0711', '3', '3', 'has', 0]\n",
      "['ABC19980108.1830.0711', '3', '4', 'lost', 0]\n",
      "['ABC19980108.1830.0711', '3', '5', 'twenty', 0]\n",
      "['ABC19980108.1830.0711', '3', '6', 'six', 0]\n",
      "['ABC19980108.1830.0711', '3', '7', 'percent', 0]\n",
      "['ABC19980108.1830.0711', '3', '8', 'of', 0]\n",
      "['ABC19980108.1830.0711', '3', '9', 'its', 0]\n",
      "['ABC19980108.1830.0711', '3', '10', 'value', 0]\n",
      "['ABC19980108.1830.0711', '3', '11', '.', 0]\n",
      "['ABC19980108.1830.0711', '4', '0', 'In', 0]\n",
      "['ABC19980108.1830.0711', '4', '1', 'Singapore', 0]\n",
      "['ABC19980108.1830.0711', '4', '2', ',', 0]\n",
      "['ABC19980108.1830.0711', '4', '3', 'stocks', 0]\n",
      "['ABC19980108.1830.0711', '4', '4', 'hit', 0]\n",
      "['ABC19980108.1830.0711', '4', '5', 'a', 0]\n",
      "['ABC19980108.1830.0711', '4', '6', 'five', 1]\n",
      "['ABC19980108.1830.0711', '4', '7', 'year', 1]\n",
      "['ABC19980108.1830.0711', '4', '8', 'low', 0]\n",
      "['ABC19980108.1830.0711', '4', '9', '.', 0]\n",
      "['ABC19980108.1830.0711', '5', '0', 'In', 0]\n",
      "['ABC19980108.1830.0711', '5', '1', 'the', 0]\n",
      "['ABC19980108.1830.0711', '5', '2', 'Philippines', 0]\n",
      "['ABC19980108.1830.0711', '5', '3', ',', 0]\n",
      "['ABC19980108.1830.0711', '5', '4', 'a', 0]\n",
      "['ABC19980108.1830.0711', '5', '5', 'four', 1]\n",
      "['ABC19980108.1830.0711', '5', '6', 'year', 1]\n",
      "['ABC19980108.1830.0711', '5', '7', 'low', 0]\n",
      "['ABC19980108.1830.0711', '5', '8', '.', 0]\n",
      "['ABC19980108.1830.0711', '6', '0', 'And', 0]\n",
      "['ABC19980108.1830.0711', '6', '1', 'in', 0]\n",
      "['ABC19980108.1830.0711', '6', '2', 'Hong', 0]\n",
      "['ABC19980108.1830.0711', '6', '3', 'Kong', 0]\n",
      "['ABC19980108.1830.0711', '6', '4', ',', 0]\n",
      "['ABC19980108.1830.0711', '6', '5', 'a', 0]\n",
      "['ABC19980108.1830.0711', '6', '6', 'three', 0]\n",
      "['ABC19980108.1830.0711', '6', '7', 'percent', 0]\n",
      "['ABC19980108.1830.0711', '6', '8', 'drop', 0]\n",
      "['ABC19980108.1830.0711', '6', '9', '.', 0]\n",
      "['ABC19980108.1830.0711', '7', '0', 'More', 0]\n",
      "['ABC19980108.1830.0711', '7', '1', 'problems', 0]\n",
      "['ABC19980108.1830.0711', '7', '2', 'in', 0]\n",
      "['ABC19980108.1830.0711', '7', '3', 'Hong', 0]\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "import collections\n",
    "import math \n",
    "import random\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "\n",
    "\n",
    "# preparing trainind Data\n",
    "count_line = 0\n",
    "training_list = []\n",
    "label_value=\"\"\n",
    "\n",
    "def search(x,y,z,lst,idx):         # x -> paragraph_id  ; y-> line_no ; z-> token_no\n",
    "    #print(\"index : \",idx)\n",
    "    for i in range(idx,len(lst)):\n",
    "        check_list = lst[i]\n",
    "        #print(check_list)\n",
    "        #newl = check_list\n",
    "        para_id = check_list[0]\n",
    "        line_no = check_list[1]\n",
    "        token_no = check_list[2]\n",
    "        \n",
    "        nn = []\n",
    "        #print(x,'=',para_id,y,line_no,z,'=',token_no)\n",
    "        if para_id==x and line_no==y and token_no==z:\n",
    "            #newl.append(\"timex3\")\n",
    "            label_value = 1\n",
    "            #prin(check_list)\n",
    "            idx = i + 1\n",
    "            #print(check_list,label_value)\n",
    "            string = \"\"\n",
    "            for j in range(len(check_list)):\n",
    "                single_val = check_list[j]\n",
    "                single_val = single_val.rstrip(\"\\n\")\n",
    "                nn.append(single_val)\n",
    "                #string += single_val + \"\\t\"\n",
    "            #print(string,\"\\t\",label_value) #\n",
    "            nn.append(label_value)\n",
    "            training_list.append(nn)\n",
    "            return idx\n",
    "        else:\n",
    "            label_value = 0\n",
    "            #prin(check_list)\n",
    "            #newl.append(\"O\")\n",
    "            string = \"\"\n",
    "            for j in range(len(check_list)):\n",
    "                single_val = check_list[j]\n",
    "                single_val = single_val.rstrip(\"\\n\")\n",
    "                nn.append(single_val)\n",
    "                #string += single_val + \"\\t\"\n",
    "            #print(string,\"\\t\",nvalue) #\n",
    "            nn.append(label_value)\n",
    "            training_list.append(nn)\n",
    "            #training_list.append(newl)\n",
    "\n",
    "            \n",
    "            \n",
    "def make_word_List():\n",
    "    words = []\n",
    "    \n",
    "    for i in range(len(training_list)):\n",
    "        #print(training_list[i])\n",
    "        ks = training_list[i]\n",
    "        string = ks[3]\n",
    "        isYes = False\n",
    "        for j in range(len(string)):\n",
    "            ch = string[j]\n",
    "            if ch >='a' and ch <='z':\n",
    "                isYes = True\n",
    "                break\n",
    "            elif ch >='A' and ch <='Z':\n",
    "                isYes = True\n",
    "                break\n",
    "            elif ch >='0' and ch <='9':\n",
    "                isYes = True\n",
    "                break\n",
    "        if isYes == True:\n",
    "            words.append(string)\n",
    "        else:\n",
    "            words.append(\"UNK\")\n",
    "                     \n",
    "    return words\n",
    "\n",
    "def main():\n",
    "    motherlist = []\n",
    "    #clone = [] \n",
    "    extent_list = []\n",
    "    idx=0\n",
    "    #  time_extent.tab = extent_list\n",
    "    with open(\"Dataset\\\\tempeval-training-2\\\\english\\\\data\\\\timex-extents.tab\",\"r\") as timex:\n",
    "        for line in timex:\n",
    "            lst = line.split(\"\\t\")\n",
    "            extent_list.append(lst)\n",
    "            #print(lst)\n",
    "    #try1 = basSegmentation.tab = motherlist\n",
    "    with open(\"Dataset\\\\tempeval-training-2\\\\english\\\\data\\\\base-segmentation.tab\",\"r\") as opn:\n",
    "        for line in opn:\n",
    "            lst = line.split(\"\\t\")\n",
    "            motherlist.append(lst)\n",
    "            #clone.append(lst)\n",
    "            #print(lst)\n",
    "    for i in range(len(extent_list)):\n",
    "        nlst = extent_list[i]\n",
    "        #print(nlst)\n",
    "        x = nlst[0]\n",
    "        y = nlst[1]\n",
    "        z = nlst[2]\n",
    "        #nlst[3] = nlst[3].rstrip(\"\\n\")\n",
    "        idx = search(x,y,z,motherlist,idx)\n",
    "        #print(i)\n",
    "        #print(x,y,z)\n",
    "    checked_line_number = len(motherlist)\n",
    "    #print(idx,checked_line_number)\n",
    "    if idx < checked_line_number :\n",
    "        for i in range(idx,checked_line_number):\n",
    "            remain_list = motherlist[i]\n",
    "            out = \"\"\n",
    "            nn=[]\n",
    "            label_value = 0\n",
    "            for j in range(len(remain_list)):\n",
    "                single_val = remain_list[j]\n",
    "                single_val = single_val.rstrip(\"\\n\")\n",
    "                nn.append(single_val)\n",
    "                #out += xxt + \"\\t\"\n",
    "            nn.append(label_value)\n",
    "            #print(nn)\n",
    "            training_list.append(nn)\n",
    "            #print(out,\"\\t\",\"O\") #\n",
    "    print(\"DONE!!!\")\n",
    "    global words \n",
    "    words = make_word_List()\n",
    "    print(len(words))\n",
    "\n",
    "if __name__== '__main__':\n",
    "    main()\n",
    "\n",
    "sep = \"====================\"    \n",
    "print(sep)\n",
    "\n",
    "print(words[:15])\n",
    "\n",
    "for i in range(len(training_list)):\n",
    "    if(training_list[i][2] == \"0\" ):\n",
    "        count_line = count_line + 1 \n",
    "        \n",
    "#Line Information \n",
    "print(sep)\n",
    "print(count_line)\n",
    "print(sep)\n",
    "word_per_line = len(words)/count_line\n",
    "print(sep)\n",
    "print(word_per_line)\n",
    "\n",
    "\n",
    "paragraph_list = []\n",
    "for k in range(len(training_list)):\n",
    "    paragraph_list.append(training_list[k][0])\n",
    "    \n",
    "\n",
    "\n",
    "print(sep)\n",
    "for k in range(100):\n",
    "    print(training_list[k]) \n",
    "print(sep)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give Integer Id to tokens and paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53450\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "====================\n",
      "7948\n",
      "====================\n",
      "Most common words : [('UNK', 6035), ('the', 2283), ('of', 1370), ('to', 1287), ('a', 1175), ('in', 860), ('and', 850), (\"'s\", 589), ('million', 528), ('said', 527)] len : 7948\n",
      "====================\n",
      "Sample data [758, 1, 72, 1538, 0, 16, 7, 2475, 109, 3, 28, 552, 416, 1899, 240, 179, 10, 574, 0, 11, 240, 6644, 14, 1, 1053, 233, 6, 1, 443, 4322] ---------- 53450\n",
      "====================\n",
      "162\n",
      "====================\n",
      "sample_paragraph [33, 33, 33, 33, 33, 33] ------- 53450\n",
      "====================\n",
      "[[33, '0', '0', 758, 0, 'On'], [33, '0', '1', 1, 0, 'the'], [33, '0', '2', 72, 0, 'other'], [33, '0', '3', 1538, 0, 'hand'], [33, '0', '4', 0, 0, ','], [33, '0', '5', 16, 0, 'it'], [33, '0', '6', 7, 0, \"'s\"], [33, '0', '7', 2475, 0, 'turning'], [33, '0', '8', 109, 0, 'out'], [33, '0', '9', 3, 0, 'to'], [33, '0', '10', 28, 0, 'be'], [33, '0', '11', 552, 0, 'another'], [33, '0', '12', 416, 0, 'very'], [33, '0', '13', 1899, 0, 'bad'], [33, '0', '14', 240, 0, 'financial'], [33, '0', '15', 179, 1, 'week'], [33, '0', '16', 10, 0, 'for'], [33, '0', '17', 574, 0, 'Asia'], [33, '0', '18', 0, 0, '.'], [33, '1', '0', 11, 0, 'The']]\n",
      "====================\n",
      "[[33, '0', '0', 758, 0, 'On', 758], [33, '0', '1', 1, 0, 'the', 1], [33, '0', '2', 72, 0, 'other', 72], [33, '0', '3', 1538, 0, 'hand', 1538], [33, '0', '4', 0, 0, ',', 0], [33, '0', '5', 16, 0, 'it', 16], [33, '0', '6', 7, 0, \"'s\", 7], [33, '0', '7', 2475, 0, 'turning', 2475], [33, '0', '8', 109, 0, 'out', 109], [33, '0', '9', 3, 0, 'to', 3], [33, '0', '10', 28, 0, 'be', 28], [33, '0', '11', 552, 0, 'another', 552], [33, '0', '12', 416, 0, 'very', 416], [33, '0', '13', 1899, 0, 'bad', 1899], [33, '0', '14', 240, 0, 'financial', 240], [33, '0', '15', 179, 1, 'week', 179], [33, '0', '16', 10, 0, 'for', 10], [33, '0', '17', 574, 0, 'Asia', 574], [33, '0', '18', 0, 0, '.', 0], [33, '1', '0', 11, 0, 'The', 11]]\n"
     ]
    }
   ],
   "source": [
    "################### No Need Under this ###########################################\n",
    "vocbulary_size = 53450\n",
    "print(len(words))\n",
    "    \n",
    "print(\"--------------------------------------------------------------\")\n",
    "print(\"--------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "def process_dataset(words):\n",
    "    \n",
    "    #count = [['UNK' , -1]] \n",
    "    count =[]\n",
    "    count.extend(collections.Counter(words).most_common (vocbulary_size-1))\n",
    "    dictionary = dict() \n",
    "    \n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "        \n",
    "      \n",
    "    data = []\n",
    "#     unk_count = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]     \n",
    "#         else:\n",
    "#             index = 0\n",
    "#             unk_count = unk_count + 1\n",
    "        data.append(index)\n",
    "    #print(unk_count)\n",
    "#    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = process_dataset(words)\n",
    "\n",
    "para_num , total_para , para_dictionary , para_reverse_dictionary = process_dataset(paragraph_list)\n",
    "# data_test , count_test , dictionary_test , reverse_dictionary_test = process_dataset(test_words)\n",
    "\n",
    "        \n",
    "\n",
    "print(sep)\n",
    "print(len(count))\n",
    "print(sep)\n",
    "\n",
    "print('Most common words :', count[:10],'len :', len(count))\n",
    "print(sep)\n",
    "print('Sample data',data[:30] ,'----------' , len(data) )\n",
    "\n",
    "print(sep)\n",
    "print(len(total_para))\n",
    "print(sep)\n",
    "\n",
    "print('sample_paragraph' , para_num[:6] ,'-------' , len(para_num))\n",
    "print(sep)\n",
    "\n",
    "print(training_list[:20])\n",
    "\n",
    "### FINAL DATA LABELING\n",
    "# Replacing words with integer id and replacing paragraph with integer id \n",
    "print(sep)\n",
    "for cnt in range(len(training_list)):\n",
    "    token = training_list[cnt][3] \n",
    "    training_list[cnt].append(token)\n",
    "    training_list[cnt][0] = para_num[cnt]\n",
    "    training_list[cnt][3] = data[cnt]\n",
    "\n",
    "print(training_list[:20])\n",
    "\n",
    "    \n",
    "# #######################################################\n",
    "# # Separate every Sentence\n",
    "\n",
    "\n",
    "\n",
    "# para_line_list = []\n",
    "# line_line_list = []\n",
    "# line_token_list = []\n",
    "# line_list = []\n",
    "# label_line_list = []\n",
    "\n",
    "\n",
    "# single_para_line = []\n",
    "# single_line_line = []\n",
    "# single_token_line = [] \n",
    "# single_line = [] \n",
    "# single_label_line =[]\n",
    "\n",
    "# for indx in range(len(training_list)):\n",
    "    \n",
    "#     if( training_list[indx][2] == \"0\"):\n",
    "        \n",
    "#         para_line_list.append(single_para_line)\n",
    "#         line_line_list.append(single_line_line)\n",
    "#         line_token_list.append(single_token_line)\n",
    "#         line_list.append(single_line)\n",
    "#         label_line_list.append(single_label_line)\n",
    "         \n",
    "#         single_para_line = []\n",
    "#         single_line_line = []\n",
    "#         single_token_line = []\n",
    "#         single_line = []\n",
    "#         single_label_line = []\n",
    "        \n",
    "#         single_para_line.append(training_list[indx][0])\n",
    "#         single_line_line.append(training_list[indx][1])\n",
    "#         single_token_line.append(training_list[indx][2])\n",
    "#         single_line.append(training_list[indx][3])\n",
    "#         single_label_line.append(training_list[indx][4])\n",
    "        \n",
    "#     else:\n",
    "           \n",
    "#         single_para_line.append(training_list[indx][0])\n",
    "#         single_line_line.append(training_list[indx][1])\n",
    "#         single_token_line.append(training_list[indx][2])\n",
    "#         single_line.append(training_list[indx][3])\n",
    "#         single_label_line.append(training_list[indx][4])\n",
    "        \n",
    "\n",
    "# para_line_list.append(single_para_line)\n",
    "# line_line_list.append(single_line_line)\n",
    "# line_token_list.append(single_token_line)\n",
    "# line_list.append(single_line)\n",
    "# label_line_list.append(single_label_line)\n",
    "\n",
    "\n",
    "# para_line_list.remove(para_line_list[0])\n",
    "# line_line_list.remove(line_line_list[0])\n",
    "# line_token_list.remove(line_token_list[0])\n",
    "# line_list.remove(line_list[0])\n",
    "# label_line_list.remove(label_line_list[0])\n",
    "\n",
    "# ######################################################\n",
    "# # Remove NonTemporal Sentence \n",
    "# print(sep)\n",
    "# print(sep)\n",
    "# count_timex_line = 0 \n",
    "\n",
    "# for ln in range(len(line_list)):\n",
    "#     if 'timex3' not in label_line_list[ln]:\n",
    "#         continue\n",
    "#     count_timex_line = count_timex_line + 1\n",
    "#     print(sep)\n",
    "#     print(para_line_list[ln])\n",
    "#     print(line_line_list[ln])\n",
    "#     print(line_token_list[ln])\n",
    "#     print(line_list[ln])\n",
    "#     print(label_line_list[ln])\n",
    "\n",
    "# print(sep)\n",
    "# print(sep)\n",
    "# print(count_timex_line)\n",
    "# print(sep)\n",
    "# ######################################################\n",
    "\n",
    "\n",
    "# for pk in range(10):\n",
    "#     print(training_list[pk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
